{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute loss\n",
    "1. compute all of confidence and loc loss\n",
    "2. compute positive confidence and loc loss (with y_true[:, :, -8] = 1 means non_background), loss got lessen\n",
    "3. find minimun from num_neg_of_batches as num_neg_batch\n",
    "4. get top-k confidences from batches. (k = num_neg_batch)\n",
    "5. get negative loss\n",
    "6. sum of and normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: loss strategy\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_classes = 21\n",
    "alpha = 1.0\n",
    "neg_pos_ratio = 3.0\n",
    "background_label_id = 0\n",
    "negatives_for_hard = 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# common offset calculation between y_true and y_pred\n",
    "# smooth offset loss\n",
    "def l1_smooth_loss(y_true, y_pred):\n",
    "    abs_loss = tf.abs(y_true - y_pred)\n",
    "    sq_loss = 0.5 * (y_true - y_pred)**2\n",
    "    # get coordinates from sq_loss as row and (abs_loss - 0.5) as column\n",
    "    l1_loss = tf.where(tf.less(abs_loss, 1.0), sq_loss, abs_loss - 0.5)\n",
    "    # calculate mean of each row\n",
    "    return tf.reduce_mean(l1_loss, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# softmax\n",
    "def softmax_loss(y_true, y_pred):\n",
    "    # given probability of prediction as [1e-15, 1 - 1e-15]\n",
    "    y_pred = tf.maximum(tf.minimum(y_pred, 1 - 1e-15), 1e-15)\n",
    "    # mean by row\n",
    "    loss = - tf.reduce_mean(y_true * tf.log(y_pred), axis=-1)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_loss(y_true, y_pred):\n",
    "    batch_size = tf.shape(y_true)[0]\n",
    "    num_boxes = tf.to_float(tf.shape(y_true)[1])\n",
    "    \n",
    "    # 1. \n",
    "    # loss for all priors\n",
    "    # class\n",
    "    conf_loss = softmax_loss(y_true[:, :, 4:-8], y_pred[:, :, 4:-8])\n",
    "    # coordinates of box\n",
    "    loc_loss = l1_smooth_loss(y_true[:, :, :4], y_pred[:, :, :4])\n",
    "    \n",
    "    # 2.\n",
    "    # get positives loss\n",
    "    pos_loc_loss = tf.reduce_sum(loc_loss * y_true[:, :, -8], axis=1)\n",
    "    pos_conf_loss = tf.reduce_sum(conf_loss * y_true[:, :, -8], axis=1)\n",
    "    \n",
    "    # mean all batches, shape is (batch, 1)\n",
    "    mean_pos_of_batches = tf.reduce_mean(y_true[:, :, -8], axis=-1)\n",
    "    \n",
    "    # get negatives loss, we panalize only confidence\n",
    "    # traverse each of batches to get num of negatives\n",
    "    \n",
    "    # 3.\n",
    "    # get num of neg of batches\n",
    "    num_neg_of_batches = tf.minimum(neg_pos_ratio * mean_pos_of_batches, num_boxes - mean_pos_of_batches)\n",
    "    pos_num_neg_mask = tf.greater(num_neg_of_batches, 0)\n",
    "    has_min = tf.to_float(tf.reduce_any(pos_num_neg_mask))\n",
    "    \n",
    "    # [num_neg, negatives_for_hard (if True)]\n",
    "    num_neg_of_batches = tf.concat(values=[num_neg_of_batches, [(1-has_min) * negatives_for_hard]], axis=0)\n",
    "    # find min_num among batches as batch size of num_neg\n",
    "    num_neg_batch = tf.reduce_min(tf.boolean_mask(num_neg_of_batches, tf.greater(num_neg_of_batches, 0)))\n",
    "    num_neg_batch = tf.to_int32(num_neg_batch)\n",
    "    \n",
    "    # 4.\n",
    "    # find max confidence probability\n",
    "    confs_start = 4 + background_label_id + 1\n",
    "    confs_end = confs_start + num_classes - 1\n",
    "    max_confs = tf.reduce_max(y_pred[:, :, confs_start:confs_end], axis=2)\n",
    "    \n",
    "    # get idx of top-k confidences \n",
    "    _, indices = tf.nn.top_k(max_confs * (1-y_true[:, :, -8]), k=num_neg_batch)\n",
    "    # shape of (batch,1)\n",
    "    batch_idx = tf.expand_dims(tf.range(0, batch_size), 1)\n",
    "    batch_idx = tf.tile(batch_idx, (1, num_neg_batch))\n",
    "    # negative idx\n",
    "    full_indices = (tf.reshape(batch_idx, [-1]) * tf.to_int32(num_boxes) \n",
    "                    + tf.reshape(indices, [-1]))\n",
    "    \n",
    "    neg_conf_loss = tf.gather(tf.reshape(conf_loss, [-1]), full_indices)\n",
    "    neg_conf_loss = tf.reshape(neg_conf_loss, [batch_size, num_neg_batch])\n",
    "    neg_conf_loss = tf.reduce_sum(neg_conf_loss, axis=1)\n",
    "    \n",
    "    # 5.\n",
    "    # sum of\n",
    "    total_loss = pos_conf_loss + neg_conf_loss\n",
    "    total_loss /= (num_pos + tf.to_float(num_neg_batch))\n",
    "    \n",
    "    num_pos = tf.where(tf.not_equal(num_pos, 0), num_pos, tf.ones_like(num_pos))\n",
    "    \n",
    "    total_loss += (alpha * pos_loc_loss) / num_pos\n",
    "    return total_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
